{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "from util import alpaca_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf you are not using a compiled llama.cpp\\n\\nllm = Llama(\\n    model_path=\\'model-unsloth.Q4_K_M.gguf\\',\\n    n_ctx=2048,  # Context length to use\\n    n_threads=1024,            # Number of CPU threads to use\\n    n_gpu_layers=32        # Number of model layers to offload to GPU\\n)\\n\\n## Generation kwargs\\ngeneration_kwargs = {\\n    \"max_tokens\":128,\\n    \"stop\":[\"</s>\"],\\n    \"echo\":False, # Echo the prompt in the output\\n    \"top_k\":10 # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\\n}\\n\\nprompt = alpaca_prompt.format(\\n    \"请用中文回答\", # instruction\\n    \"爸爸再婚，我是不是就有了个新娘？\", # input\\n    \"\", # output\\n)\\n\\n\\nres = llm(prompt, **generation_kwargs) # Res is a dictionary\\nprint(res[\"choices\"][0][\"text\"])\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "If you are not using a compiled llama.cpp\n",
    "\n",
    "llm = Llama(\n",
    "    model_path='model-unsloth.Q4_K_M.gguf',\n",
    "    n_ctx=2048,  # Context length to use\n",
    "    n_threads=1024,            # Number of CPU threads to use\n",
    "    n_gpu_layers=32        # Number of model layers to offload to GPU\n",
    ")\n",
    "\n",
    "## Generation kwargs\n",
    "generation_kwargs = {\n",
    "    \"max_tokens\":128,\n",
    "    \"stop\":[\"</s>\"],\n",
    "    \"echo\":False, # Echo the prompt in the output\n",
    "    \"top_k\":10 # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\n",
    "}\n",
    "\n",
    "prompt = alpaca_prompt.format(\n",
    "    \"请用中文回答\", # instruction\n",
    "    \"爸爸再婚，我是不是就有了个新娘？\", # input\n",
    "    \"\", # output\n",
    ")\n",
    "\n",
    "\n",
    "res = llm(prompt, **generation_kwargs) # Res is a dictionary\n",
    "print(res[\"choices\"][0][\"text\"])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "请用中文回答\n",
      "### Input:\n",
      "爸爸再婚，我是不是就有了个新娘？\n",
      "### Response:\n",
      "不是的，你并没有新娘。你爸爸的新女朋友只是你的爸爸的新伴侣，你没有新娘。<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# Use compiled llama.cpp for much faster inference\n",
    "# Switch to `model-unsloth.Q4_K_M.gguf` for faster & cheaper inference\n",
    "import subprocess\n",
    "\n",
    "MODEL_PATH = \"/home/rong/Documents/_Projects/Llama3-retard/model-unsloth.F16.gguf\"\n",
    "INPUT = alpaca_prompt.format(\n",
    "    \"请用中文回答\", # instruction\n",
    "    \"爸爸再婚，我是不是就有了个新娘？\", # input\n",
    "    \"\", # output\n",
    ")\n",
    "\n",
    "# Construct the command to run the bash script\n",
    "command = [\n",
    "    \"./llama.cpp/main\",\n",
    "    \"-m\", MODEL_PATH,\n",
    "    \"-p\", INPUT,\n",
    "    \"-n\", \"128\"\n",
    "]\n",
    "\n",
    "# Execute the command and capture its output\n",
    "try:\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    if process.returncode == 0:\n",
    "        print(\"Output:\")\n",
    "        \n",
    "        print(stdout.decode())\n",
    "    else:\n",
    "        print(\"Error occurred:\")\n",
    "        print(stderr.decode())\n",
    "except Exception as e:\n",
    "    print(\"Error occurred:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "us",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
